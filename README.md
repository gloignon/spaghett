# üéÅ happybirthday üç∞

happybirthday is a python tool to extract surprisal-based features from text.
You can also send the repo link to colleages and friends interested in computational linguistics as a nice gesture for their birthday.

## Features
* Works with AR and masked-token models from Hugging Face
* Extracts surprisal, entropy, and more! 
* [MLM only] PLL or L2R scoring available (Kauf & Ivanova, 2023). Results identical to minicons (Misra, 2022), tested in multiple situations: see test/test_minicons_compare.py.
* Can output top probability tokens and their surprisal ("counterfactual surprisal"), i.e. what the LLM thought would be the next/missing token.
* [AR only] lookahead: Can output the n next predicted tokens with the highest probability (i.e. what the LLM computed would be the continuation). Choice of greedy algorithm or beam search.
* Multilayer surprisal (see Kuribayashi et al., 2025; Li et al., 2021). You can specify "all" and happybirthday will autodetect the layers for you.
* Temperature scaling (see Liu et al., 2024).
* Extra left context: you can provide a common context file for semantic continuity, it will be prepended to each sentence's context window.
<<<<<<< HEAD
* Robust handling of special characters (e.g., `#`, `_`) and diatritics: tokens and decoded text are preserved so punctuation and underscores are not lost or mis-decoded.
* Amazing Command Line Interface (CLI).
* Programmatic API too, you can easily import the scoring functions into your own project.
* Supports .tsv or .parquet output.
* Autogenerated filenames. Don't feel like specifying an output name? Specify a folder and happybirthday will generate a super long but informative name.
* Special tokens included in output, in case you need them. Column is_special will be 1 if special token, 0 if not.
=======
* Can predict the n most probably next words, choice of greedy algorithm or beam search (will assemble next subtokens to reconstitute the word).
* Handles accented characters.
* Simple Command Line Interface, and scoring functions can easily be loaded from your own code.
* Streams results per document to disk to reduce memory footprint and keep partial progress if a run stops.
>>>>>>> new_features

# Similar work
* [minicons for python](https://github.com/kanishkamisra/minicons) also does AR and masked token models with PLL or L2R, but no entropy.
* [pangoling for R](https://docs.ropensci.org/pangoling/) Uses python internally. Does AR and masked too, but surprisal scores only.
* [psychformers for python](https://github.com/jmichaelov/PsychFormers) Supports different types of models, but only does surprisal scores for now.
* [TemperatureScaling4RTs](https://github.com/TongLiu-github/TemperatureScaling4RTs) Source code for the original temperature scaling surprisal paper (Liu et al., 2024)
* [lm-scorer](https://github.com/simonepri/lm-scorer) Focus is on scoring whole sentences, only surprisal scores.
* [text for R](https://cran.r-project.org/web/packages/text/index.html) Does word embeddings computations, not surprisal-based features.

## Installation
* You will need python. Install if you don't have it already.
* Install the libraries in requirements.txt:
```bash
pip install -r requirements.txt
```

## How to use

Input is a .tsv file. Expected columns are `doc_id` and `text` (in documents mode), or `doc_id`, `sentence_id` and `sentence` (in sentences mode). 
The first row is expected to be the column headers. Extra columns in your input file are ignored, and required columns can appear in any order. 
For sentences mode, both `sentence` and `text` are accepted as valid column names for the sentence content.

### Command Line Interface

```bash
python src/cli.py --input_file <file> --mode <ar|mlm> --model <model_name> [options]
```

#### Required Arguments
- `--input_file`: Path to input TSV file. Expected structure is either documents or sentences:
  - Documents format: columns `doc_id` and `text`
  - Sentences format: columns `doc_id`, `sentence_id` and `sentence` (or `text`)
  - Extra columns are ignored; required columns can appear in any order
- `--mode`: Model type - `ar` (autoregressive/GPT-style) or `mlm` (masked/BERT-style)
- `--model`: HuggingFace model name (e.g., `gpt2`, `bert-base-uncased`, `almanach/camembert-base`)


#### Optional Arguments
- `--output_file`: Output path (default: auto-generated with timestamp)
  - Can be a specific filename: `results.tsv`
  - Can be a folder path: `./results/` (auto-generates filename inside your folder)
- `--output_format`: Output format, either `tsv` (default) or `parquet`
- `--format`: Input format - `documents` or `sentences` (default: `sentences`)
- `--max_sentence_words`: Split sentences longer than this many whitespace-separated words before scoring (default: disabled)
- `--left_context_file`: Path to text file for left context (prepended to each sentence)
- `--top_k`: Number of top predictions to output (default: `3`, use `0` to disable)
- `--top_k_cf_surprisal`: If set, output counterfactual surprisal for each top-k prediction (pred_alt columns will be token|surprisal)
- `--layers`: List of layer indices to compute surprisal from (e.g. `--layers 0 5 11`). Option `--layers all` is also possible. If not specified, only the last layer is used and only `surprisal_bits`/`entropy_bits` columns are written.

#### AR Mode Options (GPT-style models)
- `--lookahead_n`: Number of continuation tokens to generate (default: `3`, use `0` to disable)
- `--lookahead_strategy`: Generation strategy - `greedy` or `beam` (default: `greedy`)
- `--beam_width`: Beam width for beam search (default: `3`, only used with `--lookahead_strategy beam`)
- `--resume`: Resume from an existing output file (skip already-scored `doc_id`s and append new rows)

#### MLM Mode Options (BERT-style models)
- `--pll_metric`: Scoring variant - `original` or `within_word_l2r` (default: `original`)
- `--mlm_batch_size`: Mini-batch size for per-token masks when scoring layers; lower values reduce peak memory (default: `0`, process all at once)

### Examples

**Basic usage (AR model):**
```bash
python src/cli.py --input_file data.tsv --mode ar --model gpt2
```

**French MLM with L2R scoring:**
```bash
python src/cli.py --input_file in/demo_sentences.tsv --mode mlm --model cmarkea/distilcamembert-base --pll_metric within_word_l2r
```

**Documents format with context:**
```bash
python src/cli.py --input_file docs.tsv --format documents --mode ar --model gpt2 --left_context_file context.txt
```

**Output to specific folder:**
```bash
python src/cli.py --input_file data.tsv --mode mlm --model bert-base-uncased --output_file ./results/
```

Layer-level surprisal works for AR and MLM: add `--layers` with specific indices or `--layers all` to dump surprisal and entropy at each transformer layer (new columns such as `layer3_surprisal_bits` will be appended).

**Surprisal by specific layers:**
```bash
python src/cli.py --input_file data.tsv --mode ar --model gpt2 --layers 0 5 11
```

**AR with beam search lookahead:**
```bash
python src/cli.py --input_file data.tsv --mode ar --model gpt2 --lookahead_n 5 --lookahead_strategy beam --beam_width 3
```


#### Main Results File

The output .tsv or parquet file contains:
- `doc_id`, `sentence_id`, `token_index`: Identifiers
- `token`: Raw token (with special characters like `‚ñÅ`, `ƒ†`)
- `token_decoded`: Human-readable token
- `is_special`: Flag for special tokens (BOS, EOS, CLS, SEP, etc.)
- `surprisal_bits`, `entropy_bits`: Surprisal and entropy from the last layer (default)
- `layer{idx}_surprisal_bits`, `layer{idx}_entropy_bits`: Surprisal and entropy for each requested layer (if `--layers` is used)
- `pred_alt_1` to `pred_alt_N`: Top-k alternative predictions (N = `--top_k`)
- `pred_next_1` to `pred_next_M`: Lookahead predictions (M = `--lookahead_n`, AR mode only)

**Note**: All special tokens are included in output. Filter using `is_special` column in post-processing if needed.

## Direct Function Calls

You can also import and use the core functions directly in Python, for instance, assuming the scorer.py script is in the same folder:

```python
from scorer import process_sentences

# Standard scoring with predictions
results = process_sentences(
    sentences=["Hello world.", "How are you?"],
    mode='ar',
    model_name='gpt2',
    top_k=5,
    lookahead_n=3,
    progress=True
)
```

## Testing

Test the basics:
```bash
python test/test_basics.py
```

Compare with minicons (MLM L2R scoring):
```bash
python test/test_minicons_compare.py
```

Test the multiple layer analysis mode:
```bash
python test/test_layered_surprisal.py
```

## TODO

* Create R wrapper/package
* Add different context window modes (previous sentence, previous n words, full context, etc.)
* Performance-optimized version with batching and GPU acceleration
* Support for additional model architectures

## Bibliography

* Kauf, C., & Ivanova, A. (2023). A better way to do masked language model scoring. arXiv preprint arXiv:2305.10588.
* Kuribayashi, T., Oseki, Y., Taieb, S. B., Inui, K., & Baldwin, T. (2025). Large language models are human-like internally. arXiv preprint arXiv:2502.01615.
* Li, B., Zhu, Z., Thomas, G., Xu, Y., & Rudzicz, F. (2021). How is BERT surprised? Layerwise detection of linguistic anomalies. arXiv preprint arXiv:2105.07452.
<<<<<<< HEAD
* Liu, T., ≈†krjanec, I., & Demberg, V. (2024, August). Temperature-scaling surprisal estimates improve fit to human reading times‚Äìbut does it do so for the ‚Äúright reasons‚Äù?. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 9598-9619).
* Misra, K. (2022). minicons: Enabling flexible behavioral and representational analyses of transformer language models. arXiv preprint arXiv:2203.13112.
=======
* Misra, K. (2022). minicons: Enabling flexible behavioral and representational analyses of transformer language models. arXiv preprint arXiv:2203.13112.
>>>>>>> new_features
